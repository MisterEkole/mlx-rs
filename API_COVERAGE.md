# API Coverage

This document tracks the implementation status of MLX features in mlx-rs. It's regularly updated as new functionality is added.

**Last Updated:** February 2026

## Implementation Status Overview

- âœ… **Implemented** - Feature is complete and tested
- ğŸš§ **In Progress** - Currently being developed
- ğŸ“‹ **Planned** - Scheduled for future implementation
- âŒ **Not Planned** - Not currently on the roadmap

---

## Core Array Operations

### Array Creation & Manipulation
| Feature | Status | Notes |
|---------|--------|-------|
| Array creation from slices | âœ… | Fully supported |
| Array from scalar values | âœ… | |
| zeros, ones, full | âœ… | |
| arange, linspace | âœ… | |
| eye, identity | âœ… | |
| Array indexing | âœ… | Basic and advanced indexing |
| Array slicing | âœ… | Multi-dimensional slicing |
| reshape | âœ… | |
| transpose | âœ… | |
| flatten | âœ… | |
| squeeze, expand_dims | âœ… | |
| concatenate, stack | âœ… | |
| split | ğŸ“‹ | Planned |
| pad | ğŸ“‹ | Planned |
| repeat, tile | ğŸ“‹ | Planned |

### Element-wise Operations
| Feature | Status | Notes |
|---------|--------|-------|
| Addition, subtraction | âœ… | |
| Multiplication, division | âœ… | |
| Power operations | âœ… | |
| abs, negative, sign | âœ… | |
| exp, log, log2, log10 | âœ… | |
| sqrt, square | âœ… | |
| Trigonometric (sin, cos, tan, etc.) | âœ… | |
| Inverse trig (arcsin, arccos, etc.) | âœ… | |
| Hyperbolic functions | ğŸ“‹ | Planned |
| ceil, floor, round | âœ… | |
| Comparison operators | âœ… | |
| Logical operators | âœ… | |
| where (conditional selection) | âœ… | |

### Reduction Operations
| Feature | Status | Notes |
|---------|--------|-------|
| sum | âœ… | With axis support |
| mean | âœ… | |
| var, std | ğŸ“‹ | Planned |
| min, max | âœ… | |
| argmin, argmax | ğŸ“‹ | Planned |
| all, any | ğŸ“‹ | Planned |
| logsumexp | ğŸ“‹ | Planned |
| cumsum, cumprod | ğŸ“‹ | Planned |

### Broadcasting & Shape Operations
| Feature | Status | Notes |
|---------|--------|-------|
| Automatic broadcasting | âœ… | |
| broadcast_to | âœ… | |
| broadcast_arrays | ğŸ“‹ | Planned |
| swapaxes, moveaxis | ğŸ“‹ | Planned |

### Matrix Operations
| Feature | Status | Notes |
|---------|--------|-------|
| Matrix multiplication (matmul) | âœ… | Optimized for Apple Silicon |
| Dot product | âœ… | |
| Batch matrix multiplication | âœ… | |
| Outer product | ğŸ“‹ | Planned |

---

## Linear Algebra (`mlx.linalg`)

| Feature | Status | Notes |
|---------|--------|-------|
| inv (matrix inverse) | âœ…  | |
| norm (vector/matrix norms) | âœ…  | |
| svd (Singular Value Decomposition) | âœ…  |  |
| eig, eigh (Eigenvalues) | âœ… |  |
| qr (QR Decomposition) | âœ…  | |
| cholesky | âœ… | Planned |
| solve (linear systems) | âœ… | |
| solve_triangular | âœ…  | |
| det, slogdet (Determinant) | âœ…  | |
| pinv (Pseudo-inverse) | âœ…  | |

---

## FFT Operations (`mlx.fft`)

| Feature | Status | Notes |
|---------|--------|-------|
| fft, ifft (1D) | âœ… | |
| rfft, irfft (Real FFT) | âœ… | |
| fft2, ifft2 (2D) | âœ…  | Planned |
| fftn, ifftn (N-dimensional) |âœ… | |
| fftshift, ifftshift | âœ… | |
| fftfreq, rfftfreq | âœ… | |

---

## Random Number Generation (`mlx.random`)

| Feature | Status | Notes |
|---------|--------|-------|
| key, split (PRNG key management) | ğŸ“‹ | Planned |
| uniform | ğŸ“‹ | Planned |
| normal | ğŸ“‹ | Planned |
| bernoulli | ğŸ“‹ | Planned |
| categorical | ğŸ“‹ | Planned |
| randint | ğŸ“‹ | Planned |
| permutation, shuffle | ğŸ“‹ | Planned |
| multivariate_normal | ğŸ“‹ | Planned |
| truncated_normal | ğŸ“‹ | Planned |

---

## Neural Networks (`mlx.nn`)

### Layers
| Feature | Status | Notes |
|---------|--------|-------|
| Linear (Dense) | âœ… | Fully featured |
| Conv1d | âœ… | |
| Conv2d | âœ… | |
| Conv3d | ğŸ“‹ | Planned |
| ConvTranspose1d | ğŸ“‹ | Planned |
| ConvTranspose2d | ğŸ“‹ | Planned |
| Embedding | âœ… | |
| Dropout | âœ… | |
| BatchNorm | ğŸ“‹ | Planned|
| LayerNorm | âœ…| |
| GroupNorm | ğŸ“‹ | Planned |
| InstanceNorm | ğŸ“‹ | Planned |
| RMSNorm | âœ… | Planned |

### Recurrent Layers
| Feature | Status | Notes |
|---------|--------|-------|
| RNN | âœ… | |
| LSTM | âœ… | |
| GRU | âœ… | |
| Bidirectional wrappers | âœ… ||

### Pooling Layers
| Feature | Status | Notes |
|---------|--------|-------|
| MaxPool1d | âœ…| |
| MaxPool2d | âœ…| |
| AvgPool1d |âœ…| |
| AvgPool2d | âœ…| |
| AdaptiveAvgPool | ğŸ“‹ | Planned |
| AdaptiveMaxPool | ğŸ“‹ | Planned |

### Activation Functions
| Feature | Status | Notes |
|---------|--------|-------|
| ReLU | âœ… | |
| GELU | âœ… | |
| SiLU (Swish) | ğŸ“‹| |
| Sigmoid | âœ… | |
| Tanh | âœ… | |
| Softmax | âœ… | |
| LogSoftmax | âœ… |Planned |
| LeakyReLU | ğŸ“‹| Planned|
| ELU | ğŸ“‹|Planned |
| PReLU | ğŸ“‹ | Planned |
| Mish | ğŸ“‹ | Planned |

### Attention Mechanisms
| Feature | Status | Notes |
|---------|--------|-------|
| MultiHeadAttention | ğŸ“‹ | Planned|
| Scaled Dot-Product Attention | ğŸ“‹| Planned|
| Cross Attention | ğŸ“‹| |
| Rotary Position Embeddings (RoPE) | ğŸ“‹ | Planned |
| Alibi | ğŸ“‹ | Planned |
| Flash Attention | ğŸ“‹ | Planned |

### Transformer Components
| Feature | Status | Notes |
|---------|--------|-------|
| TransformerEncoder | ğŸ“‹ | |
| TransformerDecoder | ğŸ“‹| |
| TransformerEncoderLayer | ğŸ“‹| |
| TransformerDecoderLayer | ğŸ“‹ | |

### Loss Functions
| Feature | Status | Notes |
|---------|--------|-------|
| MSE Loss | âœ… | |
| Cross Entropy Loss | âœ… |Planned|
| Binary Cross Entropy | ğŸ“‹| Planned|
| L1 Loss | ğŸ“‹| Planned|
| Smooth L1 Loss | ğŸ“‹ | Planned |
| KL Divergence | ğŸ“‹ | Planned |
| Cosine Embedding Loss | ğŸ“‹ | Planned |

---

## Optimizers (`mlx.optimizers`)

| Feature | Status | Notes |
|---------|--------|-------|
| SGD | âœ… | With momentum support |
| Adam | âœ… | |
| AdamW | âœ…||
| AdaGrad |âœ…| |
| RMSprop | âœ…| |
| Lion | âœ…| |
| Adafactor | âœ…| |

### Learning Rate Schedulers
| Feature | Status | Notes |
|---------|--------|-------|
| StepLR | ğŸ“‹ | Planned |
| ExponentialLR | ğŸ“‹ | Planned |
| CosineAnnealingLR | ğŸ“‹ | Planned |
| ReduceLROnPlateau | ğŸ“‹ | Planned |
| OneCycleLR | ğŸ“‹ | Planned |
| Warmup schedules | ğŸ“‹ | Planned |

---

## Automatic Differentiation

| Feature | Status | Notes |
|---------|--------|-------|
| grad | âœ… | Compute gradients |
| value_and_grad | âœ… | Value and gradient together |
| vjp (Vector-Jacobian Product) | ğŸ“‹ | Planned |
| jvp (Jacobian-Vector Product) | ğŸ“‹ | Planned |
| jacobian | ğŸ“‹ | Planned |
| hessian | ğŸ“‹ | Planned |
| stop_gradient | ğŸ“‹ | Planned |
| Custom gradient functions | ğŸ“‹ | Planned |

---

## Function Transformations

| Feature | Status | Notes |
|---------|--------|-------|
| vmap (Vectorization) | ğŸ“‹ | Planned |
| compile (JIT Compilation) | ğŸ“‹ | Planned |
| checkpoint (Gradient Checkpointing) | ğŸ“‹ | Planned |

---

## Quantization

| Feature | Status | Notes |
|---------|--------|-------|
| 4-bit quantization | ğŸ“‹| Planned|
| 8-bit quantization | ğŸ“‹ | Planned|
| quantize, dequantize | ğŸ“‹| Planned |
| QuantizedLinear | ğŸ“‹ | Planned |
| QuantizedEmbedding | ğŸ“‹ | Planned |
| Quantized Attention | ğŸ“‹ | Planned |
| Dynamic quantization | ğŸ“‹ | Planned |
| Static quantization | ğŸ“‹ | Planned |

---

## File I/O

### Serialization Formats
| Feature | Status | Notes |
|---------|--------|-------|
| NumPy format (save/load) | ğŸš§ | In progress |
| Safetensors format | ğŸš§ | In progress |
| GGUF format | ğŸ“‹ | Planned (llama.cpp compat) |
| Pickle format | ğŸ“‹ | Planned |

### Model Management
| Feature | Status | Notes |
|---------|--------|-------|
| Save model weights | ğŸš§ | In progress |
| Load model weights | ğŸš§ | In progress |
| Checkpoint management | ğŸ“‹ | Planned |
| Partial loading | ğŸ“‹ | Planned |
| Model sharding | ğŸ“‹ | Planned |

---

## Distributed Computing

| Feature | Status | Notes |
|---------|--------|-------|
| distributed.init | ğŸ“‹ | Planned |
| all_reduce | ğŸ“‹ | Planned |
| all_gather | ğŸ“‹ | Planned |
| all_sum | ğŸ“‹ | Planned |
| broadcast | ğŸ“‹ | Planned |
| Multi-GPU support | ğŸ“‹ | Planned |
| Data parallelism | ğŸ“‹ | Planned |
| Model parallelism | ğŸ“‹ | Planned |

---

## Stream Management

| Feature | Status | Notes |
|---------|--------|-------|
| Stream creation | âœ… | |
| Stream synchronization | âœ… | |
| Default stream management | âœ… | |
| Stream context managers | âœ… | |
| Async operations | ğŸ“‹| |

---

## Utilities

| Feature | Status | Notes |
|---------|--------|-------|
| eval (Force evaluation) | âœ… | |
| Device management (cpu/gpu) | âœ… | |
| Memory pool management | ğŸ“‹ | Planned |
| depends (Operation dependencies) | ğŸ“‹ | Planned |
| tree_map, tree_flatten | ğŸ“‹ | Planned |
| Profiling utilities | ğŸ“‹ | Planned |

---

## Platform Support

| Platform | Status | Notes |
|---------|--------|-------|
| macOS M1 | âœ… | Fully supported |
| macOS M2 | âœ… | Fully supported |
| macOS M3 | âœ… | Fully supported |
| macOS M4 | âœ… | Fully supported |
| Intel Macs | âŒ | Not supported by MLX |
| Linux | âŒ | Not supported by MLX |
| Windows | âŒ | Not supported by MLX |

---

## Contributing to API Coverage

If you'd like to contribute to implementing any of these features:

1. Check the status in this document
2. Open an issue on GitHub to discuss the implementation
3. Reference the [MLX C API documentation](https://github.com/ml-explore/mlx-c)
4. Follow the contribution guidelines in CONTRIBUTING.md
5. Update this document when features are completed

---

## Changelog

### February 2026
- âœ… Completed core array operations
- âœ… Implemented neural network module
- âœ… Added automatic differentiation (grad, value_and_grad)
- âœ… Implemented SGD and Adam optimizers
